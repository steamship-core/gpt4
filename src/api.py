import json
import logging
import openai
from pydantic import Field
from steamship import Steamship, Block, Tag, SteamshipError, MimeTypes
from steamship.agents.schema import Tool
from steamship.agents.schema.functions import (
    OpenAIFunction,
    FunctionProperty,
    JSONType,
    FunctionParameters,
)
from steamship.data.tags.tag_constants import TagKind, RoleTag, TagValueKey, ChatTag
from steamship.invocable import Config, InvocableResponse, InvocationContext
from steamship.plugin.capabilities import (
    RequestedCapabilities,
    SystemPromptSupport,
    SupportLevel,
    ConversationSupport,
    FunctionCallingSupport,
)
from steamship.plugin.generator import Generator
from steamship.plugin.inputs.raw_block_and_tag_plugin_input import (
    RawBlockAndTagPluginInput,
)
from steamship.plugin.outputs.plugin_output import (
    UsageReport,
    OperationType,
    OperationUnit,
)
from steamship.plugin.outputs.raw_block_and_tag_plugin_output import (
    RawBlockAndTagPluginOutput,
)
from steamship.plugin.request import PluginRequest
from tenacity import (
    after_log,
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    before_sleep_log,
    wait_exponential_jitter,
)
from typing import Any, Dict, List, Optional, Type, Mapping

VALID_MODELS_FOR_BILLING = [
    "gpt-4",
    "gpt-4-0314",
    "gpt-4-0613",
    "gpt-4-32k",
    "gpt-4-32k-0613",
    "gpt-3.5-turbo",
    "gpt-3.5-turbo-0613",
    "gpt-3.5-turbo-16k",
    "gpt-3.5-turbo-16k-0613",
]


SUPPORT_MAP = {
    SystemPromptSupport: SupportLevel.NATIVE,
    ConversationSupport: SupportLevel.NATIVE,
    FunctionCallingSupport: SupportLevel.NATIVE
}


def tool_as_openai_function(tool: Tool) -> OpenAIFunction:
    text_property_schema = FunctionProperty(
        type=JSONType.string,
        description="text prompt for a function.",
    )

    uuid_property_schema = FunctionProperty(
        type=JSONType.string,
        description="UUID for a Steamship Block. Used to refer to a non-textual input generated by another "
        "function. Example: c2f6818c-233d-4426-9dc5-f3c28fa33068",
    )

    params = FunctionParameters(
        properties={"text": text_property_schema, "uuid": uuid_property_schema},
    )

    return OpenAIFunction(
        name=tool.name,
        # TODO Alter this?  How do we not break folks?
        description=tool.human_description,
        parameters=params,
    )


class GPT4Plugin(Generator):
    """
    Plugin for generating text using OpenAI's GPT-4 model.
    """

    class GPT4PluginConfig(Config):
        openai_api_key: str = Field(
            "",
            description="An openAI API key to use. If left default, will use Steamship's API key.",
        )
        max_tokens: int = Field(
            256,
            description="The maximum number of tokens to generate per request. Can be overridden in runtime options.",
        )
        model: Optional[str] = Field(
            "gpt-4-0613",
            description="The OpenAI model to use. Can be a pre-existing fine-tuned model.",
        )
        temperature: Optional[float] = Field(
            0.4,
            description="Controls randomness. Lower values produce higher likelihood / more predictable results; "
                        "higher values produce more variety. Values between 0-1.",
        )
        top_p: Optional[int] = Field(
            1,
            description="Controls the nucleus sampling, where the model considers the results of the tokens with "
                        "top_p probability mass. Values between 0-1.",
        )
        presence_penalty: Optional[int] = Field(
            0,
            description="Control how likely the model will reuse words. Positive values penalize new tokens based on "
                        "whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Number between -2.0 and 2.0.",
        )
        frequency_penalty: Optional[int] = Field(
            0,
            description="Control how likely the model will reuse words. Positive values penalize new tokens based on "
                        "their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. Number between -2.0 and 2.0.",
        )
        moderate_output: bool = Field(
            True,
            description="Pass the generated output back through OpenAI's moderation endpoint and throw an exception "
                        "if flagged.",
        )
        max_retries: int = Field(
            8, description="Maximum number of retries to make when generating."
        )
        request_timeout: Optional[float] = Field(
            600,
            description="Timeout for requests to OpenAI completion API. Default is 600 seconds.",
        )
        n: Optional[int] = Field(
            1, description="How many completions to generate for each prompt."
        )
        default_role: str = Field(
            RoleTag.USER.value,
            description="The default role to use for a block that does not have a Tag of kind='role'",
        )
        default_system_prompt: str = Field(
            "", description="System prompt that will be prepended before every request"
        )

    @classmethod
    def config_cls(cls) -> Type[Config]:
        return cls.GPT4PluginConfig

    config: GPT4PluginConfig

    def __init__(
            self,
            client: Steamship = None,
            config: Dict[str, Any] = None,
            context: InvocationContext = None,
    ):
        # Load original api key before it is read from TOML, so we know to restrict models for billing
        original_api_key = config.get("openai_api_key", "")
        super().__init__(client, config, context)
        if original_api_key == "" and self.config.model not in VALID_MODELS_FOR_BILLING:
            raise SteamshipError(
                f"This plugin cannot be used with model {self.config.model} while using Steamship's API key. Valid models are {VALID_MODELS_FOR_BILLING}"
            )
        openai.api_key = self.config.openai_api_key

    def prepare_message(self, block: Block) -> Optional[Dict[str, str]]:
        role = None
        name = None
        function_selection = False

        for tag in block.tags:
            if tag.kind == TagKind.ROLE:
                # this is a legacy way of extracting roles
                role = tag.name

            if tag.kind == TagKind.CHAT and tag.name == ChatTag.ROLE:
                if values := tag.value:
                    if role_name := values.get(TagValueKey.STRING_VALUE, None):
                        role = role_name

            if tag.kind == TagKind.ROLE and tag.name == RoleTag.FUNCTION:
                if values := tag.value:
                    if fn_name := values.get(TagValueKey.STRING_VALUE, None):
                        name = fn_name

            if tag.kind == "function-selection":
                function_selection = True

            if tag.kind == "name":
                name = tag.name

        if block.mime_type == MimeTypes.STEAMSHIP_PLUGIN_FUNCTION_CALL_INVOCATION:
            invocation = FunctionCallingSupport.FunctionCallInvocation.from_block(block)
            return {
                "role": "assistant",  # This does not use our enums, because those are for our purposes, and this is what OpenAI wants.
                "content": None,
                "function_call": {
                    "name": invocation.tool_name,
                    "arguments": {
                        arg_name: value for arg_name, value in invocation.args
                    }
                }
            }

        if block.mime_type == MimeTypes.STEAMSHIP_PLUGIN_FUNCTION_CALL_RESULT:
            call_result = FunctionCallingSupport.FunctionCallResult.from_block(block)
            return {
                "role": "function",
                "name": call_result.tool_name,
                "content": call_result.result
            }

        if role is None:
            role = self.config.default_role

        if role not in ["function", "system", "assistant", "user"]:
            logging.warning(f"unsupported role {role} found in message. skipping...")
            return None

        if role == "function" and not name:
            name = "unknown"  # protect against missing function names

        if function_selection:
            return {"role": RoleTag.ASSISTANT.value, "content": None, "function_call": json.loads(block.text)}

        if name:
            return {"role": role, "content": block.text, "name": name}

        return {"role": role, "content": block.text}

    def prepare_messages(self, blocks: List[Block], ) -> List[Dict[str, str]]:
        messages = []
        # TODO (SHIP-854) coalesce this with conversation history
        if self.config.default_system_prompt != "":
            messages.append(
                {"role": RoleTag.SYSTEM, "content": self.config.default_system_prompt}
            )
        # TODO: remove is_text check here when can handle image etc. input
        messages.extend(
            [
                msg for msg in
                (self.prepare_message(block) for block in blocks if block.text is not None and block.text != "")
                if msg is not None
            ]
        )
        return messages

    def generate_with_retry(
            self, user: str, messages: List[Dict[str, str]], requested_capabilities: Optional[RequestedCapabilities], options: Dict
    ) -> (List[Block], List[UsageReport]):
        """Call the API to generate the next section of text."""
        logging.info(
            f"Making OpenAI GPT-4 chat completion call on behalf of user with id: {user}"
        )
        options = options or {}
        stopwords = options.get("stop", None)
        if requested_capabilities:
            function_calling_support = requested_capabilities.get(FunctionCallingSupport)
            if function_calling_support:
                functions = [tool_as_openai_function(tool) for tool in function_calling_support.functions]
        else:
            # Legacy behavior
            functions = options.get("functions", None)

        @retry(
            reraise=True,
            stop=stop_after_attempt(self.config.max_retries),
            wait=wait_exponential_jitter(jitter=5),
            before_sleep=before_sleep_log(logging.root, logging.INFO),
            retry=(
                    retry_if_exception_type(openai.error.Timeout)
                    | retry_if_exception_type(openai.error.APIError)
                    | retry_if_exception_type(openai.error.APIConnectionError)
                    | retry_if_exception_type(openai.error.RateLimitError)
                    | retry_if_exception_type(
                ConnectionError
            )  # handle 104s that manifest as ConnectionResetError
            ),
            after=after_log(logging.root, logging.INFO),
        )
        def _generate_with_retry() -> Any:
            kwargs = dict(
                model=self.config.model,
                messages=messages,
                user=user,
                presence_penalty=self.config.presence_penalty,
                frequency_penalty=self.config.frequency_penalty,
                max_tokens=self.config.max_tokens,
                stop=stopwords,
                n=self.config.n,
                temperature=self.config.temperature,
            )
            if functions:
                kwargs = {**kwargs, "functions": functions}

            logging.info("calling open ai chatcompletion create",
                         extra={"messages": messages, "functions": functions})
            return openai.ChatCompletion.create(**kwargs)

        openai_result = _generate_with_retry()
        logging.info(
            "Retry statistics: " + json.dumps(_generate_with_retry.retry.statistics)
        )

        # Fetch text from responses
        generation_blocks = []
        for choice in openai_result["choices"]:
            message = choice["message"]
            role = message["role"]
            mime_type = None
            if function_call := message.get("function_call"):
                if requested_capabilities:
                    content = FunctionCallingSupport.FunctionCallInvocation(
                        tool_name=function_call["name"],
                        arguments=function_call["arguments"]
                    ).json()
                    mime_type = FunctionCallingSupport.FunctionCallInvocation.MIME_TYPE
                else:
                    # Legacy behavior
                    content = json.dumps({"function_call": function_call})
            else:
                content = message.get("content", "")

            generation_blocks.append(Block(text=content, role=role, mime_type=mime_type))

        # for token usage tracking, we need to include not just the token usage, but also completion id
        # that will allow proper usage aggregration for n > 1 cases
        usage = openai_result["usage"]
        usage["completion_id"] = openai_result["id"]

        usage_reports = [
            UsageReport(
                operation_type=OperationType.RUN,
                operation_unit=OperationUnit.PROMPT_TOKENS,
                operation_amount=usage["prompt_tokens"],
                audit_id=usage["completion_id"],
            ),
            UsageReport(
                operation_type=OperationType.RUN,
                operation_unit=OperationUnit.SAMPLED_TOKENS,
                operation_amount=usage["completion_tokens"],
                audit_id=usage["completion_id"],
            ),
        ]

        return generation_blocks, usage_reports

    @staticmethod
    def _flagged(messages: List[Dict[str, str]]) -> bool:
        input_text = "\n\n".join(
            [json.dumps(value) for role_dict in messages for value in role_dict.values()]
        )
        moderation = openai.Moderation.create(input=input_text)
        return moderation["results"][0]["flagged"]

    def run(
            self, request: PluginRequest[RawBlockAndTagPluginInput]
    ) -> InvocableResponse[RawBlockAndTagPluginOutput]:
        """Run the text generator against all the text, combined"""

        self.config.extend_with_dict(request.data.options, overwrite=True)
        requested_capabilities = RequestedCapabilities(SUPPORT_MAP)
        capability_response = requested_capabilities.extract_from_blocks(request.data.blocks)
        if not capability_response:
            requested_capabilities = None
        messages = self.prepare_messages(request.data.blocks)
        if self.config.moderate_output and self._flagged(messages):
            raise SteamshipError(
                "Sorry, this content is flagged as inappropriate by OpenAI."
            )
        user_id = self.context.user_id if self.context is not None else "testing"
        generated_blocks, usage_reports = self.generate_with_retry(
            messages=messages, user=user_id, options=request.data.options, requested_capabilities=requested_capabilities
        )
        if capability_response:
            # TODO append to file?
            generated_blocks.append(capability_response.to_block())

        return InvocableResponse(
            data=RawBlockAndTagPluginOutput(
                blocks=generated_blocks, usage=usage_reports
            )
        )
